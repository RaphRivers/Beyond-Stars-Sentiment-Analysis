{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0df8e5",
   "metadata": {},
   "source": [
    "# Sentiment, Statistical Analysis, and Hypothesis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c364a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment analysis, also known as opinion mining, is a powerful technique used to determine the sentiment or emotional tone expressed in text data, such as user reviews in your research. It involves analyzing and classifying the polarity of the text, typically into positive, negative, or neutral categories, to gauge the sentiment conveyed by the author. In the context of your research on the impact of review sentiments and star ratings on business patronage, sentiment analysis is a crucial component for understanding how users perceive and evaluate businesses. Below, I'll detail the methods you can use for sentiment analysis and how to create a sentiment scale for your text sentiment data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da1ba4",
   "metadata": {},
   "source": [
    "### Data preprocessing, tokenization, feature extraction, post processing and evaluation\n",
    "This preprocessing plays a pivotal role in conducting sentiment analysis by facilitating the cleaning and normalization of the data sample reviews text data, thereby enhancing its suitability for analysis. This crucial step encompasses various techniques aimed at converting raw text data into a format conducive to analysis. Common text preprocessing techniques include tokenization, removal of stop words such as \"and,\" \"the,\" \"of,\" and \"it\", and lemmatization used to reduce \"Lemma\" infected words based on their intending meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfff06a",
   "metadata": {},
   "source": [
    "### Import need libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55692c4c",
   "metadata": {},
   "source": [
    "Download all the nltk corpus for the first time to ensure that all the necessary data is available for natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d106d613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\raphr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import natural language processing tool kit NLTK libraries for data preprocessing and tokenization\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re \n",
    "nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f57e4",
   "metadata": {},
   "source": [
    "### Import generated random sample business dataset for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1a72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read random sample csv into a dataframe\n",
    "biz_sample = pd.read_csv('csv/biz_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e43b2b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   business_id   25000 non-null  object\n",
      " 1   name          25000 non-null  object\n",
      " 2   address       24703 non-null  object\n",
      " 3   city          25000 non-null  object\n",
      " 4   state         25000 non-null  object\n",
      " 5   postal_code   24999 non-null  object\n",
      " 6   review_count  25000 non-null  int64 \n",
      " 7   is_open       25000 non-null  int64 \n",
      " 8   categories    25000 non-null  object\n",
      " 9   review_id     25000 non-null  object\n",
      " 10  user_id       25000 non-null  object\n",
      " 11  stars         25000 non-null  int64 \n",
      " 12  useful        25000 non-null  int64 \n",
      " 13  funny         25000 non-null  int64 \n",
      " 14  cool          25000 non-null  int64 \n",
      " 15  text          25000 non-null  object\n",
      " 16  date          25000 non-null  object\n",
      "dtypes: int64(6), object(11)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Inspect dataframe\n",
    "biz_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04ea29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the reviews column 'text' to 'review'\n",
    "biz_sample.rename(columns = {'text': 'review'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de6e7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WxZ2Ua5hb7g3hZqWjw5k6w</td>\n",
       "      <td>615 Pizza and Pasta</td>\n",
       "      <td>5337 Charlotte Ave</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>TN</td>\n",
       "      <td>37209</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>dOITfBm-j5Ts5kYx1J0UAw</td>\n",
       "      <td>mOapNjIy3ynfYuNt5Pvexg</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>First let me start off by saying I wasn't look...</td>\n",
       "      <td>2020-01-22 06:46:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LCitqsu9DV1RjGn7EIzzIQ</td>\n",
       "      <td>Outback Steakhouse</td>\n",
       "      <td>610 Old York Rd</td>\n",
       "      <td>Jenkintown</td>\n",
       "      <td>PA</td>\n",
       "      <td>19046</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>RueeEC-0KfShSmr5x6DP7Q</td>\n",
       "      <td>MUGaNRi8f9mzsEqzw98YOQ</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dirty..dirty..dirty place.  Floors very greasy...</td>\n",
       "      <td>2016-05-08 04:01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 name             address  \\\n",
       "0  WxZ2Ua5hb7g3hZqWjw5k6w  615 Pizza and Pasta  5337 Charlotte Ave   \n",
       "1  LCitqsu9DV1RjGn7EIzzIQ   Outback Steakhouse     610 Old York Rd   \n",
       "\n",
       "         city state postal_code  review_count  is_open   categories  \\\n",
       "0   Nashville    TN       37209            38        1  Restaurants   \n",
       "1  Jenkintown    PA       19046           147        1  Restaurants   \n",
       "\n",
       "                review_id                 user_id  stars  useful  funny  cool  \\\n",
       "0  dOITfBm-j5Ts5kYx1J0UAw  mOapNjIy3ynfYuNt5Pvexg      5       1      0     0   \n",
       "1  RueeEC-0KfShSmr5x6DP7Q  MUGaNRi8f9mzsEqzw98YOQ      2       0      0     0   \n",
       "\n",
       "                                              review                 date  \n",
       "0  First let me start off by saying I wasn't look...  2020-01-22 06:46:53  \n",
       "1  Dirty..dirty..dirty place.  Floors very greasy...  2016-05-08 04:01:00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_sample.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d40bf6c",
   "metadata": {},
   "source": [
    "*Next we preprocess sample dataframe 'review' field for sentiment analysis using multi-model fusion approach.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f7f8e",
   "metadata": {},
   "source": [
    "### Methods for Sentiment Analysis\n",
    "\n",
    "#### Lexicon-Based Sentiment Analysis: \n",
    "Lexicon-based approaches involve using predefined sentiment lexicons or dictionaries that contain words and phrases associated with positive and negative sentiment. We use the NLTK Vader sentiment analyzer which employs a predefined set of rules and heuristics to assess the sentiment of a given text. These rules primarily rely on lexical and syntactic characteristics of the text, taking into account the occurrence of positive or negative words and phrases.Each word is assigned a polarity score, and the sentiment of a text is determined by summing these scores. For example, the word \"excellent\" might have a high positive score, while \"terrible\" has a high negative score. This method is relatively simple and efficient.\n",
    "\n",
    "#### Machine Learning-Based Sentiment Analysis: \n",
    "Machine learning methods involve training models on labeled data to predict the sentiment of text. Common techniques include using algorithms like Naive Bayes, Support Vector Machines (SVM), or deep learning models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). Training data consists of text samples with known sentiment labels (positive, negative, neutral). Once trained, the model can predict sentiment for unseen text.\n",
    "\n",
    "#### Rule-Based Sentiment Analysis: \n",
    "Rule-based approaches involve designing custom rules or heuristics to identify sentiment in text. These rules can be based on grammatical structures, word patterns, or specific keywords. Rule-based methods are highly interpretable and allow for fine-grained control over sentiment analysis.\n",
    "\n",
    "#### Hybrid Approaches: \n",
    "Hybrid methods combine elements of lexicon-based, machine learning, and rule-based approaches to improve accuracy. They may use lexicons as features for machine learning models or incorporate rule-based heuristics within a machine learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcdcae9",
   "metadata": {},
   "source": [
    "### NLTK data preprocessing combining lemmatization and stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a77161",
   "metadata": {},
   "source": [
    "By combining lemmatization and stemming in NLTK pipelines we aim to reduce words to their base or root form to increase the coverage and improve text matching and similarity calculations. It helps use in treating variations of words as similar, contributing to better text comparisons. Stemming is a more aggressive approach that trims the end of words to a common root. THus, it may not always produce valid words, but it can handle variations well. While lemmatization, on the other hand, tends to produce valid words but may not cover as many variations as stemming. This combine approach maintain interpretability while still capturing some of the aggressive reductions and is beneficial in helping us understand the sentiment of the core words in the reviews. Also using both lemmatization and stemming contribute to creating better feature representations for our text data and helps in reducing dimensionality by capturing the essential semantic information for further machine learning application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21ebf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to 'preprocess' to tokenize the dataframe review field, then remove step words such as 'and','of','it','or'etc.\n",
    "revLemmatize = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def rev_prep(reviewText):\n",
    "    reviewText=str(reviewText) #convert all text to string.\n",
    "    reviewText=reviewText.lower() #convert all string to lower case. \n",
    "    reviewText=reviewText.replace('{html}',\"\") #replace html str instances with space.\n",
    "    cleaner=re.compile('<.*?>') #compile punctuations str pattern into cleaner object.\n",
    "    cleanText=re.sub(cleaner, '', reviewText) #replace all matching occurrences of the cleaner object pattern. \n",
    "    remove_url=re.sub(r'http\\S+', '',cleanText) #replace all matching occurrences of url.\n",
    "    remove_num=re.sub('[0-9]+', '', remove_url) #replace all occurrences of digits (0-9) in a string with a space.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #define the pattern for tokenization using re to split text into tokens based on seq. \n",
    "    revTokens_Filter = tokenizer.tokenize(remove_num) #tokenize the string stored in the variable remove_num \n",
    "    \n",
    "    #filter out tokens that have a length less than or equal to 2 characters from the revTokens_Filter list and are found in a list of English stopwords\n",
    "    filtered_text = [i for i in revTokens_Filter if len(i) > 2 if not i in stopwords.words('english')]\n",
    "    stem_text=[stemmer.stem(i) for i in filtered_text] #reduce words in filtered_text obj to their base or root form\n",
    "    lemma_words=[revLemmatize.lemmatize(i) for i in stem_text] #lemmatize each word in the stem_text list\n",
    "    return ' '.join(filtered_text) #concatenate the elements of the filtered_text list into a single string with space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd3f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the funtion to 'review' column in the sample dataframe and store processed text\n",
    "biz_sample['review'] = biz_sample['review'].apply(rev_prep)\n",
    "# To apply additional logic we will use the lambda function below\n",
    "# biz_sample['review'] = biz_sample['review'].map(lambda s:rev_prep(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38246a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WxZ2Ua5hb7g3hZqWjw5k6w</td>\n",
       "      <td>615 Pizza and Pasta</td>\n",
       "      <td>5337 Charlotte Ave</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>TN</td>\n",
       "      <td>37209</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>dOITfBm-j5Ts5kYx1J0UAw</td>\n",
       "      <td>mOapNjIy3ynfYuNt5Pvexg</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>first let start saying looking much late night...</td>\n",
       "      <td>2020-01-22 06:46:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LCitqsu9DV1RjGn7EIzzIQ</td>\n",
       "      <td>Outback Steakhouse</td>\n",
       "      <td>610 Old York Rd</td>\n",
       "      <td>Jenkintown</td>\n",
       "      <td>PA</td>\n",
       "      <td>19046</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>RueeEC-0KfShSmr5x6DP7Q</td>\n",
       "      <td>MUGaNRi8f9mzsEqzw98YOQ</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dirty dirty dirty place floors greasy sliding ...</td>\n",
       "      <td>2016-05-08 04:01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 name             address  \\\n",
       "0  WxZ2Ua5hb7g3hZqWjw5k6w  615 Pizza and Pasta  5337 Charlotte Ave   \n",
       "1  LCitqsu9DV1RjGn7EIzzIQ   Outback Steakhouse     610 Old York Rd   \n",
       "\n",
       "         city state postal_code  review_count  is_open   categories  \\\n",
       "0   Nashville    TN       37209            38        1  Restaurants   \n",
       "1  Jenkintown    PA       19046           147        1  Restaurants   \n",
       "\n",
       "                review_id                 user_id  stars  useful  funny  cool  \\\n",
       "0  dOITfBm-j5Ts5kYx1J0UAw  mOapNjIy3ynfYuNt5Pvexg      5       1      0     0   \n",
       "1  RueeEC-0KfShSmr5x6DP7Q  MUGaNRi8f9mzsEqzw98YOQ      2       0      0     0   \n",
       "\n",
       "                                              review                 date  \n",
       "0  first let start saying looking much late night...  2020-01-22 06:46:53  \n",
       "1  dirty dirty dirty place floors greasy sliding ...  2016-05-08 04:01:00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_sample.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2697c8ba",
   "metadata": {},
   "source": [
    "### Sentiment Scale\n",
    "\n",
    "Due to the computational constraint of this research we will be using lexicon based sentiment analysis. This allow us to create a sentiment scale that involves quantifying the sentiment scores obtained from the sentiment analysis methods above to provide a structured representation of the sentiment expressed in the users reviews. This scale can be used to numerically assess the sentiment of a given text, making it amenable to statistical analysis. \n",
    "\n",
    "**Polarity Scores:** Each sentiment analysis method will assign polarity scores to the text, typically on a scale from -1 (very negative) to 1 (very positive). Neutral sentiments can be assigned a score of 0. These scores represent the intensity of sentiment in the text.\n",
    "\n",
    "**Aggregation:** To create a sentiment scale, we aggregate the polarity scores from multiple methods. This can be done by averaging the scores or using a weighted average if you want to give more weight to a specific method's output.\n",
    "\n",
    "**Binning:** After aggregation, we categorize the aggregated scores into sentiment categories. For instance, scores in the range of -1 to -0.5 could be classified as \"negative,\" -0.5 to 0.5 as \"neutral,\" and 0.5 to 1 as \"positive.\"\n",
    "\n",
    "**Normalization:** Based on the analysis requirements, we choose to normalize the scale, mapping the scores to a specific range. This normalization makes the sentiment scores more interpretable and consistent across various texts.\n",
    "\n",
    "This sentiment scale, created from sentiment analysis output, will allow us to quantitatively assess the sentiment expressed in reviews, enabling us to explore the relationship between sentiment, star ratings, and their impact on business patronage in a structured and analyzable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1cbacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize NLTK setiment analyzer\n",
    "revAnalyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ad1dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign polarity score\n",
    "def getSenti(text):\n",
    "    scores = revAnalyzer.polarity_scores(text) #analyze the sentiment and provide a dictionary of polarity scores.\n",
    "    sentiment = 1 if scores['pos'] > 0 else 0 #accesses the 'pos' key in the scores dictionary that rep positive polarity\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac29de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run polarity score function\n",
    "biz_sample['sentiment'] = biz_sample['review'].apply(getSenti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7965b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sentiment scale using capture_sentiment funtion\n",
    "def capture_sentiment(revText):\n",
    "    scores = revAnalyzer.polarity_scores(revText) #use polarity scores to assign sentiment value pos, neg and neu\n",
    "    sentiment_score = scores['compound'] #normalize the combination of the positive, negative, and neutral scores \n",
    "    if sentiment_score >= 0.5:\n",
    "        sentiment_label = 'positive' #label for pos sentiment\n",
    "    elif sentiment_score <= -0.5:\n",
    "        sentiment_label = 'negative' #label for neg sentiment\n",
    "    else:\n",
    "        sentiment_label = 'neutral' #label for neu sentiment\n",
    "    \n",
    "    return sentiment_score, sentiment_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6f9650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the capture sentiment function to review field and assign scores and lables in a new columns \n",
    "biz_sample['sentiment_score'], biz_sample['sentiment_label'] = zip(*biz_sample['review'].apply(capture_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2f90b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_sample.to_csv('yelp_nltk_sentiment_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e2541",
   "metadata": {},
   "source": [
    "### NLTK classification and evaluation of model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd78a8",
   "metadata": {},
   "source": [
    "We use a confusion matrix to evaluate the performance of the NLTK model classification by comparing the predicted labels with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84c2fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion matrix module function from scikit-learn machine learning library \n",
    "from sklearn.metrics import confusion_matrix, classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953e3e4",
   "metadata": {},
   "source": [
    "### Model 2 - Sentiment Analysis with BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe20960",
   "metadata": {},
   "source": [
    "### Model 3 - Sentiment Analysis with CNN VGG-16  Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c417e",
   "metadata": {},
   "source": [
    "### Model 4 - Sentiment Analysis with Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f4b1d",
   "metadata": {},
   "source": [
    "### Model 4 - Sentiment Analysis with GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1923d6a2",
   "metadata": {},
   "source": [
    "### Model 4 - Sentiment Analysis with SPINN Hybrid tree-sequence neural networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
